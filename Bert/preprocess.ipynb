{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 절차 소개\n",
    "\n",
    "* 문장 시작 [CLS] | 문장 끝 [SEP]\n",
    "\n",
    "* MLM 방식과 NSP 방식을 통해 학습\n",
    "* MLM의 경우 전체 단어의 15%가 Mask or 엉뚱한 단어로 변경\n",
    "* 이때 비율은  [Mask] 8, 엉뚱한 단어 2\n",
    "* NSP의 경우 다음 문장을 예측하는 용도\n",
    "* NSP를 위해 하나의 row는 2개의 sentence로 구성\n",
    "\n",
    "## WordPiece Embedding \n",
    "\n",
    "* Word Piece는 context 기반, word embedding은 단어 기반\n",
    "* Bank라는 단어는 문맥에 따라 여러 의미로 쓰임. \n",
    "* ex) We Went to river Bank || I need to go to bank \n",
    "* Word Piece는 두 개의 vector를 생성한다면 word embbeding은 하나의 vector만 생성함.\n",
    "\n",
    "  <a href = 'https://medium.com/swlh/differences-between-word2vec-and-bert-c08a3326b5d1'> 출처 : WordPiece Embedding과 Word2Vec차이 </a>\n",
    "\n",
    "* WordPiece Embedding은 SubWord를 사용\n",
    "  - 단어를 보면 ##ing과 같이 구분되고 있음. 같은단어라도 단어의 조합에 따라 의미가 다르고 이를 반영하기 위한 조치임 \n",
    "\n",
    "  <img alt='img1' src='./img/img1.png' style=\"width : 400px\">\n",
    "\n",
    "  > 자주 등장하는 단어(sub-word)는 그 자체가 단위가 되고, 자주 등장하지 않는 단어(rare word)는 더 작은 sub-word로 쪼개어집니다. \n",
    "\n",
    "  <a href = 'https://happy-obok.tistory.com/23'> Bert 이해하기 : WordPiece 소개</a>\n",
    "\n",
    "  > 단어보다 더 작은 단위로 쪼개는 서브워드 토크나이저(subword tokenizer)를 사용합니다. 서브워드 토크나이저는 기본적으로 자주 등장하는 단어는 그대로 단어 집합에 추가하지만, 자주 등장하지 않는 단어는 더 작은 단위인 서브워드로 분리되어 서브워드들이 단어 집합에 추가된다는 아이디어를 갖고 있습니다.\n",
    "  >\n",
    "  > 예를 들어, embeddings라는 단어가 입력으로 들어왔을 때 BERT는 단어 집합에 해당 단어가 존재하지 않았다고 해봅시다. 서브워드 토크나이저가 아닌 토크나이저라면 여기서 OOV(out of vocabulary) 문제가 발생하지만, 서브워드 토크나이저의 경우 해당 단어를 더 쪼개려고 시도합니다. 만약 BERT의 단어 집합에 em, ##bed, ##ding, #s라는 서브워드들이 존재한다면 embeddings는 em, ##bed, ##ding, #s로 분리됩니다. 여기서 ##은 서브워드들이 단어 중간부터 등장하는 것임을 알려주기 위한 기호입니다.\n",
    "\n",
    "  <a href = 'https://moondol-ai.tistory.com/463#:~:text=BERT%EB%8A%94%20%EB%91%90%20%EA%B0%9C%EC%9D%98%20%EB%AC%B8%EC%9E%A5,%EA%B0%9C%EC%9D%98%20%EB%AC%B8%EC%9E%A5%EC%9D%B4%20%EC%A3%BC%EC%96%B4%EC%A7%91%EB%8B%88%EB%8B%A4.'> BERT의 서브워드 토크나이저: WordPiece</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "IMDB_data = pd.read_csv('./data/IMDB Dataset.csv')\n",
    "\n",
    "IMDB_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 491161/491161 [00:05<00:00, 93639.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 전처리 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [01:23<00:00, 597.85it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[646, 30, 7, 2484]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "class IMDBBertData(Dataset) :\n",
    "    # special token\n",
    "    CLS = '[CLS]' # 문장 시작\n",
    "    PAD = '[PAD]' # 빈공간\n",
    "    SEP = '[SEP]' # 문장 끝\n",
    "    MASK = '[MASK]'\n",
    "    UNK = '[UNK]'\n",
    "\n",
    "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
    "    TARGET_COLUMN = 'indices'\n",
    "\n",
    "    # nsp 용도\n",
    "    NSP_TARGET_COLUMN = 'is_next'\n",
    "    TOKEN_MASK_COLUMN = 'token_mask'\n",
    "\n",
    "    mask_percent = 0.15\n",
    "\n",
    "    OPTIMAL_LENGTH_PERCENTILE = 70\n",
    "\n",
    "\n",
    "    def __init__(self,path, ds_from=None, ds_to=None, should_include_text=False) -> None:\n",
    "        '''\n",
    "         should_include_text = True : Debug 모드 \n",
    "        '''\n",
    "        # load dataset\n",
    "        self.ds = pd.read_csv(path)['review']\n",
    "        \n",
    "        # slice dataset\n",
    "        if ds_from is not None or ds_to is not None : \n",
    "            self.ds[ds_from:ds_to]\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.counter = Counter()\n",
    "        self.vocab = None\n",
    "\n",
    "        self.optimal_sentence_length = None\n",
    "        self.should_include_text = should_include_text\n",
    "\n",
    "        # self.df(dataframe) 만들 때 column 정의\n",
    "        if should_include_text :\n",
    "            self.columns = [\n",
    "                'masked_sentence',\n",
    "                self.MASKED_INDICES_COLUMN,'sentence',\n",
    "                self.TARGET_COLUMN,\n",
    "                self.TOKEN_MASK_COLUMN,\n",
    "                self.NSP_TARGET_COLUMN\n",
    "                ]\n",
    "\n",
    "        else : \n",
    "            self.columns = [\n",
    "                self.MASKED_INDICES_COLUMN,\n",
    "                self.TARGET_COLUMN,\n",
    "                self.TOKEN_MASK_COLUMN,\n",
    "                self.NSP_TARGET_COLUMN\n",
    "                ]\n",
    "\n",
    "        # 최종 값 df에 저장\n",
    "        self.df = self.prepare_dataset()\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ''' \n",
    "        getitem은 class에서 바로 슬라이싱을 하면 실행하는 함수임\n",
    "        ex) IMDBBertData[0:1] => return (inp,attention_mask ...)\n",
    "        현재는 getitem을 활용해 trainin을 간편하게 하려는 목적임\n",
    "        '''\n",
    "\n",
    "        item = self.df.iloc[index]\n",
    "        inp = torch.Tensor(item[self.MASKED_INDICES_COLUMN]).long() # int64 타입지정\n",
    "        token_mask = torch.Tensor(item[self.TOKEN_MASK_COLUMN]).bool()\n",
    "\n",
    "        mask_target = torch.Tensor(item[self.TARGET_COLUMN]).long() # int64 타입지정\n",
    "        mask_target = mask_target.masked_fill_(token_mask, 0)\n",
    "\n",
    "\n",
    "        # 훈련 시 [PAD] 제거용으로 쓰인다고 함.\n",
    "        attention_mask = (inp==self.vocab[self.PAD]).unsqueeze(0)\n",
    "\n",
    "        if item[self.NSP_TARGET_COLUMN] == 0 :\n",
    "            t = [1,0]\n",
    "        else :\n",
    "            t = [0,1]\n",
    "        nsp_target = torch.Tensor(t)\n",
    "        return(\n",
    "            inp,attention_mask,token_mask,mask_target,nsp_target\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self) :\n",
    "        '''\n",
    "        Main Function\n",
    "        Bert에 활용 될 Dataset 만드는 function \n",
    "        '''\n",
    "\n",
    "        # vocab에 단어 저장\n",
    "        # vocab 사용법 : vocab(['here','is','the','example']) => to indices\n",
    "        # vocab.lookup_indices()\n",
    "        # vocab.lookup_token()\n",
    "\n",
    "        sentences = []\n",
    "        nsp = []\n",
    "        sentence_lens = []\n",
    "        \n",
    "        # For MLM\n",
    "        for review in self.ds :\n",
    "            review_sentences = review.split('. ')\n",
    "            sentences += review_sentences #extend 대신 + 사용해도 됨\n",
    "            self._update_length(review_sentences,sentence_lens)\n",
    "\n",
    "        self.optimal_sentence_length = self._find_optimal_sentence_length(sentence_lens)\n",
    "\n",
    "        print('vocab 생성')\n",
    "        for sentence in tqdm(sentences) : \n",
    "            s = self.tokenizer(sentence)\n",
    "            self.counter.update(s)\n",
    "\n",
    "        self._fill_vocab()\n",
    "\n",
    "        # For NSP\n",
    "        print('데이터 전처리 시작')\n",
    "        for review in tqdm(self.ds) :\n",
    "            review_sentences = review.split('. ')\n",
    "            if len(review_sentences) > 1 :\n",
    "                for i in range(len(review_sentences) - 1) :\n",
    "                    # True NSP item\n",
    "                    first, second = self.tokenizer(review_sentences[i]),self.tokenizer(review_sentences[i + 1])\n",
    "                    nsp.append(self._create_item(first,second,1))\n",
    "                    # False NSP item \n",
    "                    first,second = self._select_false_nsp_sentences(sentences)\n",
    "                    first,second = self.tokenizer(first), self.tokenizer(second)\n",
    "                    nsp.append(self._create_item(first,second,0))\n",
    "\n",
    "        df = pd.DataFrame(nsp,columns=self.columns)\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    # For MLM\n",
    "    def _update_length(self,input:list, output) :\n",
    "        '''\n",
    "        개별 문장의 단어 개수 반환\n",
    "        '''\n",
    "        for sen in input :\n",
    "            output.append(len(sen.split(' '))) \n",
    "\n",
    "        \n",
    "\n",
    "    def _find_optimal_sentence_length(self,lengths:list) :\n",
    "        '''\n",
    "        lengths = dataset에 있는 문장 길이\n",
    "        return : 70% 범위의 문장 길이\n",
    "        IMDB는 27개라고 함.\n",
    "        '''\n",
    "        arr = np.array(lengths)\n",
    "        return int(np.percentile(arr,self.OPTIMAL_LENGTH_PERCENTILE))\n",
    "\n",
    "    def _fill_vocab(self) :\n",
    "        # 2번 이상 반복되는 단어만 저장하기\n",
    "        self.vocab = vocab(self.counter, min_freq=2)  \n",
    "\n",
    "        self.vocab.insert_token(self.CLS, 0)  \n",
    "        self.vocab.insert_token(self.PAD, 1)  \n",
    "        self.vocab.insert_token(self.MASK, 2)  \n",
    "        self.vocab.insert_token(self.SEP, 3)  \n",
    "        self.vocab.insert_token(self.UNK, 4)  \n",
    "        self.vocab.set_default_index(4)\n",
    "\n",
    "    def _create_item(self,first:list, second:list, target:int = 1):\n",
    "        '''\n",
    "        NSP 훈련용으로 활용\n",
    "        1. Masked Sentence 생성\n",
    "        2. random words Sentence 생성\n",
    "        '''\n",
    "\n",
    "        # 1.masked Sentence 생성\n",
    "        updated_first, first_mask = self._preprocess_sentence(first.copy())\n",
    "        updated_second, second_mask = self._preprocess_sentence(second.copy())\n",
    "\n",
    "        nsp_sentence = updated_first + [self.SEP] + updated_second\n",
    "        nsp_indicies = self.vocab.lookup_indices(nsp_sentence)\n",
    "        inverse_token_mask = first_mask + [True] + second_mask\n",
    "\n",
    "        # 2.masked Sentence 생성\n",
    "        first,_ =self._preprocess_sentence(first.copy(), should_mask = False)\n",
    "        second,_ =self._preprocess_sentence(second.copy(), should_mask = False)\n",
    "        original_nsp_sentence = first + [self.SEP] + second \n",
    "        original_nsp_indices = self.vocab.lookup_indices(original_nsp_sentence)\n",
    "\n",
    "        if self.should_include_text :\n",
    "            return(\n",
    "                nsp_sentence,\n",
    "                nsp_indicies,\n",
    "                original_nsp_sentence,\n",
    "                original_nsp_indices,\n",
    "                inverse_token_mask,\n",
    "                target\n",
    "            )\n",
    "        else : \n",
    "            return (\n",
    "                nsp_indicies,\n",
    "                original_nsp_indices,\n",
    "                inverse_token_mask,\n",
    "                target\n",
    "            )\n",
    "\n",
    "    # For NSP Sentence\n",
    "    def _select_false_nsp_sentences(self, sentences:list) :\n",
    "        ''' \n",
    "        false NSP 만들 sentence 선택하기\n",
    "        sentences : 문장 리스트 전체\n",
    "        return : 임의로 문장 2개 선택(앞과 뒤는 연결되지 않음)\n",
    "        '''\n",
    "        sentences_len = len(sentences)\n",
    "        sentence_index = random.randint(0,sentences_len -1)\n",
    "        next_sentence_index = random.randint(0,sentences_len -1)\n",
    "\n",
    "        while next_sentence_index == sentence_index +1 : \n",
    "            next_sentence_index = random.randint(0,sentences_len -1)\n",
    "            \n",
    "        return sentences[sentence_index], sentences[next_sentence_index]\n",
    "\n",
    "    def _preprocess_sentence(self, sentence:list, should_mask :bool = True) :\n",
    "        ''' \n",
    "        mask 퍼센트(15%)를 [mask]로 바꾸는 매소드\n",
    "        sentences : 문장 리스트 전체\n",
    "        return : mask 된 문장, inverse token mask\n",
    "        '''\n",
    "        inverse_token_mask = None\n",
    "        if should_mask : \n",
    "            sentence, inverse_token_mask = self._mask_sentence(sentence)\n",
    "        sentence, inverse_token_mask = self._pad_sentence([self.CLS] + sentence,inverse_token_mask)\n",
    "\n",
    "        return sentence, inverse_token_mask\n",
    "\n",
    "    def _mask_sentence(self,sentence:list) :\n",
    "        ''' \n",
    "        mask 퍼센트(15%)를 [mask] 또는 랜덤한 단어로 바뀜\n",
    "        이때 [mask] : random_word = 8 : 2 비율\n",
    "        sentence : 문장 하나\n",
    "        return : mask 또는 random 단어 변환 된 문장, inverse token mask\n",
    "        '''\n",
    "        len_s = len(sentence)\n",
    "        # len(len_s, 27)\n",
    "        # masked 된 문장 또는 단어가 바뀐 경우 False로 변환됨\n",
    "        inverse_token_mask = [True for _ in range(max(len_s,self.optimal_sentence_length))] \n",
    "\n",
    "        mask_amount = round(len_s *self.mask_percent) \n",
    "\n",
    "        for _ in range(mask_amount) : \n",
    "            i = random.randint(0,len_s - 1)\n",
    "\n",
    "            if random.random() < 0.8 :\n",
    "                # mask_amount의 80%는 mask로 \n",
    "                sentence[i] = self.MASK\n",
    "            else :\n",
    "                # # mask_amount의 20%는 단어 바꾸기 \n",
    "                # 5인 이유는 0,1,2,3,4 토큰이 모두 special token이기 때문\n",
    "                j = random.randint(5,len(self.vocab) - 1)\n",
    "\n",
    "                # 단어 바꾸기\n",
    "                sentence[i] = self.vocab.lookup_token(j)\n",
    "\n",
    "            inverse_token_mask[i] = False\n",
    "\n",
    "        return sentence, inverse_token_mask\n",
    "\n",
    "    def _pad_sentence(self,sentence : list,inverse_token_mask : bool) :\n",
    "        len_s =len(sentence)\n",
    "\n",
    "        # self.optimal_sentence_length = 27단어\n",
    "        if len_s >= self.optimal_sentence_length :\n",
    "            s = sentence[:self.optimal_sentence_length]\n",
    "        else : \n",
    "            s = sentence + [self.PAD] * (self.optimal_sentence_length - len_s)\n",
    "        \n",
    "        if inverse_token_mask :\n",
    "            len_m = len(inverse_token_mask) \n",
    "            if len_m >= self.optimal_sentence_length :\n",
    "                inverse_token_mask = inverse_token_mask[:self.optimal_sentence_length]\n",
    "            else : \n",
    "                inverse_token_mask = inverse_token_mask + [True] * (self.optimal_sentence_length - len_m)\n",
    "\n",
    "        return s, inverse_token_mask\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = IMDBBertData('./data/IMDB Dataset.csv',should_include_text=True)\n",
    "\n",
    "a.vocab(['here','is','the','example'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "882322"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end = a.df\n",
    "len(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_sentence</th>\n",
       "      <th>masked_indices</th>\n",
       "      <th>sentence</th>\n",
       "      <th>indices</th>\n",
       "      <th>token_mask</th>\n",
       "      <th>is_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[CLS], one, of, the, [MASK], reviewers, has, ...</td>\n",
       "      <td>[0, 5, 6, 7, 2, 9, 10, 11, 12, 13, 14, 15, 2, ...</td>\n",
       "      <td>[[CLS], one, of, the, other, reviewers, has, m...</td>\n",
       "      <td>[0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...</td>\n",
       "      <td>[True, True, True, False, True, True, True, Tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[CLS], it, ', s, a, really, great, movie, [MA...</td>\n",
       "      <td>[0, 73, 20, 242, 56, 246, 240, 364, 2, 41322, ...</td>\n",
       "      <td>[[CLS], it, ', s, a, really, great, movie, wit...</td>\n",
       "      <td>[0, 73, 20, 242, 56, 246, 240, 364, 34, 56, 19...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Fal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[CLS], they, are, right, ,, [MASK], this, [MA...</td>\n",
       "      <td>[0, 24, 25, 26, 27, 2, 29, 2, 31, 32, 33, 34, ...</td>\n",
       "      <td>[[CLS], they, are, right, ,, as, this, is, exa...</td>\n",
       "      <td>[0, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34...</td>\n",
       "      <td>[True, True, True, True, False, True, False, T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[CLS], and, also, to, see, how, an, [MASK], b...</td>\n",
       "      <td>[0, 44, 624, 67, 226, 450, 87, 2, 153, 2, 4117...</td>\n",
       "      <td>[[CLS], and, also, to, see, how, an, good, but...</td>\n",
       "      <td>[0, 44, 624, 67, 226, 450, 87, 469, 153, 3618,...</td>\n",
       "      <td>[True, True, True, True, True, True, False, Tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[CLS], trust, me, ,, this, is, not, a, [MASK]...</td>\n",
       "      <td>[0, 54, 35, 27, 29, 30, 55, 56, 2, 58, 7, 59, ...</td>\n",
       "      <td>[[CLS], trust, me, ,, this, is, not, a, show, ...</td>\n",
       "      <td>[0, 54, 35, 27, 29, 30, 55, 56, 57, 58, 7, 59,...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Fal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     masked_sentence  \\\n",
       "0  [[CLS], one, of, the, [MASK], reviewers, has, ...   \n",
       "1  [[CLS], it, ', s, a, really, great, movie, [MA...   \n",
       "2  [[CLS], they, are, right, ,, [MASK], this, [MA...   \n",
       "3  [[CLS], and, also, to, see, how, an, [MASK], b...   \n",
       "4  [[CLS], trust, me, ,, this, is, not, a, [MASK]...   \n",
       "\n",
       "                                      masked_indices  \\\n",
       "0  [0, 5, 6, 7, 2, 9, 10, 11, 12, 13, 14, 15, 2, ...   \n",
       "1  [0, 73, 20, 242, 56, 246, 240, 364, 2, 41322, ...   \n",
       "2  [0, 24, 25, 26, 27, 2, 29, 2, 31, 32, 33, 34, ...   \n",
       "3  [0, 44, 624, 67, 226, 450, 87, 2, 153, 2, 4117...   \n",
       "4  [0, 54, 35, 27, 29, 30, 55, 56, 2, 58, 7, 59, ...   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  [[CLS], one, of, the, other, reviewers, has, m...   \n",
       "1  [[CLS], it, ', s, a, really, great, movie, wit...   \n",
       "2  [[CLS], they, are, right, ,, as, this, is, exa...   \n",
       "3  [[CLS], and, also, to, see, how, an, good, but...   \n",
       "4  [[CLS], trust, me, ,, this, is, not, a, show, ...   \n",
       "\n",
       "                                             indices  \\\n",
       "0  [0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...   \n",
       "1  [0, 73, 20, 242, 56, 246, 240, 364, 34, 56, 19...   \n",
       "2  [0, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34...   \n",
       "3  [0, 44, 624, 67, 226, 450, 87, 469, 153, 3618,...   \n",
       "4  [0, 54, 35, 27, 29, 30, 55, 56, 57, 58, 7, 59,...   \n",
       "\n",
       "                                          token_mask  is_next  \n",
       "0  [True, True, True, False, True, True, True, Tr...        1  \n",
       "1  [True, True, True, True, True, True, True, Fal...        0  \n",
       "2  [True, True, True, True, False, True, False, T...        1  \n",
       "3  [True, True, True, True, True, True, False, Tr...        0  \n",
       "4  [True, True, True, True, True, True, True, Fal...        1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " '[MASK]',\n",
       " 'reviewers',\n",
       " 'has',\n",
       " 'mentioned',\n",
       " 'that',\n",
       " 'after',\n",
       " 'watching',\n",
       " 'just',\n",
       " '[MASK]',\n",
       " 'oz',\n",
       " 'reinvent',\n",
       " 'you',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'be',\n",
       " 'hooked',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '[MASK]',\n",
       " 'are',\n",
       " 'right',\n",
       " ',',\n",
       " 'as',\n",
       " 'this',\n",
       " 'is',\n",
       " 'exactly',\n",
       " 'what',\n",
       " 'happened',\n",
       " 'with',\n",
       " 'me',\n",
       " '.',\n",
       " 'the',\n",
       " '[MASK]',\n",
       " 'thing',\n",
       " 'that',\n",
       " 'struck',\n",
       " 'me',\n",
       " 'about',\n",
       " 'oz',\n",
       " 'ladylike',\n",
       " 'its',\n",
       " 'dotty',\n",
       " 'and',\n",
       " 'unflinching']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장이 2개 있는 이유 NSP 구분하기 위함\n",
    "end.iloc[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "end.to_csv('./data/preprocessed_DB.csv',index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
