{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert 소개\n",
    "\n",
    "* Transformer의 encoder 부분만 활용\n",
    "\n",
    "* NLP 분야에 Fine-Tuning 개념 도입\n",
    "* Masked Language Model[MLM] 뿐만아니라 Next Sentence Prediction[NSP]를 통해 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JointEmbedding \n",
    "Bert Embedding 종류는 세가지\n",
    "\n",
    "* Token Embeddings : token을 indices로 변경\n",
    "\n",
    "* Segment Embeddings : 2개 문장의 단어를 구분하기 위해 0,1로 표시 ex) [0,0,0, ... 1,1,1]\n",
    "\n",
    "* Position Embeddings : 전체 단어의 순번 \n",
    "\n",
    "  <img alt='img0' src='./img/img0.png' style=\"width : 400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "class JointEmbedding(nn.Module) : \n",
    "\n",
    "    def __init__(self, vocab_size, size, device='cpu') :\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.device = device\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, size)\n",
    "        self.segment_emb = nn.Embedding(vocab_size, size)\n",
    "\n",
    "        self.norm =  nn.LayerNorm(size)\n",
    "\n",
    "    def forward(self,input_tensor) : \n",
    "        sentence_size = input_tensor.size(-1)\n",
    "\n",
    "        # positional embbeding\n",
    "        pos_tensor = self.attention_position(self.size, input_tensor)\n",
    "        \n",
    "        # segment embedding\n",
    "        segment_tensor = torch.zeros_like(input_tensor).to(self.device)\n",
    "\n",
    "        # embedding size의 반은 0 반은 1임\n",
    "        segment_tensor[:, sentence_size // 2 + 1:] = 1\n",
    "\n",
    "        output = self.token_emb(input_tensor) + self.segment_emb(segment_tensor) + pos_tensor\n",
    "        return self.norm(output)\n",
    "\n",
    "    def attention_position(self,dim,input_tensor) :\n",
    "        '''\n",
    "        ????\n",
    "        '''\n",
    "        # input_tensor row 크기 \n",
    "        batch_size = input_tensor.size(0)\n",
    "\n",
    "        # 문장 길이\n",
    "        sentence_size = input_tensor.size(-1)\n",
    "\n",
    "        # pos 정의 longtype = int64\n",
    "        pos = torch.arange(sentence_size, dtype=torch.long).to(self.device)\n",
    "\n",
    "        # d = sentence 내 허용 token 개수\n",
    "        d = torch.arange(dim, dtype=torch.long).to(self.device)\n",
    "        d = (2*d /dim)\n",
    "\n",
    "        # unsqueeze 공부해야할듯..\n",
    "        pos = pos.unsqueeze(1)\n",
    "        pos = pos / (1e4**d)\n",
    "\n",
    "        pos[:, ::2] = torch.sin(pos[:, ::2])\n",
    "        pos[:, 1::2] = torch.cos(pos[:, 1::2])\n",
    "\n",
    "        # *pos는 처음 보는 방식인데\n",
    "        return pos.expand(batch_size, *pos.size())\n",
    "\n",
    "# \n",
    "    def numeric_position(self,dim,input_tensor) : \n",
    "        pos_tensor = torch.arange(dim,dtype=torch.long).to(self.device)\n",
    "        return pos_tensor.expand_as(input_tensor)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_inp, dim_out):\n",
    "        super(AttentionHead, self).__init__()\n",
    "\n",
    "        self.dim_inp = dim_inp\n",
    "\n",
    "        self.q = nn.Linear(dim_inp, dim_out)\n",
    "        self.k = nn.Linear(dim_inp, dim_out)\n",
    "        self.v = nn.Linear(dim_inp, dim_out)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        query, key, value = self.q(input_tensor), self.k(input_tensor), self.v(input_tensor)\n",
    "\n",
    "        scale = query.size(1) ** 0.5\n",
    "        scores = torch.bmm(query, key.transpose(1, 2)) / scale\n",
    "\n",
    "        scores = scores.masked_fill_(attention_mask, -1e9)\n",
    "        attn = f.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(attn, value)\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, dim_inp, dim_out):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(dim_inp, dim_out) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(dim_out * num_heads, dim_inp)\n",
    "        self.norm = nn.LayerNorm(dim_inp)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        s = [head(input_tensor, attention_mask) for head in self.heads]\n",
    "        scores = torch.cat(s, dim=-1)\n",
    "        scores = self.linear(scores)\n",
    "        return self.norm(scores)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_inp, dim_out, attention_heads=4, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(attention_heads, dim_inp, dim_out)  # batch_size x sentence size x dim_inp\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim_inp, dim_out),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim_out, dim_inp),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim_inp)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        context = self.attention(input_tensor, attention_mask)\n",
    "        res = self.feed_forward(context)\n",
    "        return self.norm(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert 논문 기본 parameter\n",
    "1. Encoder = 12\n",
    "2. heads = 12\n",
    "3. Hidden Layer(=embedding size) = 768\n",
    "4. word piece = 30522(30522개 단어라는 말)\n",
    "5. Parameter = 110M\n",
    "\n",
    "\n",
    "### 110M parameter 계산하기 \n",
    "* 30522*768 = 24M(embedding 단어)\n",
    "* 12 encoder = 84M\n",
    "    - 1 encoder = 7M\n",
    "    - 세부사항은 상세링크 보기\n",
    "* Dense Weight Matrix and Bias [768, 768] = 589824, [768] = 768, (589824 + 768 = 590592)\n",
    "= 110M\n",
    "\n",
    "    <a href='https://stackoverflow.com/questions/64485777/how-is-the-number-of-parameters-be-calculated-in-bert-model'>상세 링크</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "class Bert(nn.Module) : \n",
    "    def __init__(self,vocab_size,dim_input,dim_output, attention_heads) -> None:\n",
    "        '''\n",
    "        vocab_size : input vocab total\n",
    "        dim_input : (=hidden_layer= embedding_size) 768 \n",
    "        dim_output : (=hidden_layer= embedding_size) 768\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = JointEmbedding(vocab_size, dim_input)\n",
    "        self.encoder = Encoder(dim_input, dim_output, attention_heads)\n",
    "\n",
    "        self.token_prediction_layer = nn.Linear(dim_input, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.classification_layer = nn.Linear(dim_input, 2)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask) : \n",
    "        embedded = self.embedding(input_tensor)\n",
    "        encoded = self.encoder(embedded,attention_mask)\n",
    "\n",
    "        token_predictions = self.token_prediction_layer(encoded)\n",
    "\n",
    "        # 모든 행의 첫번째 단어(embedding)\n",
    "        first_word = encoded[:, 0, :]\n",
    "\n",
    "        return self.softmax(token_predictions), self.classification_layer(first_word)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [25, 26, 27, 28, 29],\n",
       "        [50, 51, 52, 53, 54],\n",
       "        [75, 76, 77, 78, 79]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### First_word 표현 방식 이해용\n",
    "import torch.nn\n",
    "import torch\n",
    "\n",
    "a = torch.arange(0,100).reshape((4,5,5))\n",
    "\n",
    "\n",
    "a[:,0,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import time\n",
    "# dataset_load\n",
    "import dataset\n",
    "\n",
    "class BertTrainer : \n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        dataset, \n",
    "        log_dir, \n",
    "        checkpoint_dir, \n",
    "        print_progress =10, \n",
    "        print_accuracy = 50,\n",
    "        batch_size=24, \n",
    "        learning_rate = 0.005, \n",
    "        epochs = 5,\n",
    "        device = 'cpu'\n",
    "        ):\n",
    "        self.model = model  \n",
    "        self.dataset = dataset  \n",
    "        self.device = device\n",
    "\n",
    "        self.batch_size = batch_size  \n",
    "        self.epochs = epochs  \n",
    "        self.current_epoch = 0 \n",
    "\n",
    "        # data 불러오기\n",
    "        self.loader = DataLoader(self.dataset, batch_size=self.batch_size,shuffle=True) \n",
    "\n",
    "        # TensorBoard SummeryWriter 불러오기\n",
    "        self.writer = SummaryWriter(str(log_dir))  \n",
    "\n",
    "        self.checkpoint_dir = checkpoint_dir  \n",
    "\n",
    "        # NSP 용         \n",
    "        # BCEWithLogitsLoss : This loss combines a Sigmoid layer and the BCELoss in one single class.\n",
    "        # BCE = Binary Cross Entropy    \n",
    "        self.criterion = nn.BCEWithLogitsLoss().to(self.device)  \n",
    "\n",
    "        # MLM 용\n",
    "        # NLLLoss : The negative log likelihood loss. It is useful to train a classification problem with C classes.\n",
    "        self.ml_criterion = nn.NLLLoss(ignore_index=0).to(self.device)  \n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.015)\n",
    "\n",
    "        self._splitter_size = 35\n",
    "\n",
    "        self._ds_len = len(self.dataset)\n",
    "        self._batched_len = self._ds_len // self.batch_size\n",
    "\n",
    "        self._print_every = print_progress\n",
    "        self._accuracy_every = print_accuracy\n",
    "    \n",
    "    def __call__(self) :\n",
    "        # Class를 함수처럼 사용하도록 만드는 매서드\n",
    "        for self.current_epoch in range(self.current_epoch,self.epochs) :\n",
    "            loss = self.train(self.current_epoch)\n",
    "            self.save_checkpoint(self.current_epoch)\n",
    "    \n",
    "    def train(self, epoch) :\n",
    "        print(f\"epoch 시작 {epoch}\")\n",
    "\n",
    "        prev = time.time()\n",
    "\n",
    "        # gradient set\n",
    "        average_nsp_loss = 0\n",
    "        average_mlm_loss = 0\n",
    "\n",
    "        # Gradient Descent 시작 \n",
    "        for i, value in enumerate(self.loader) :\n",
    "            index = i + 1\n",
    "\n",
    "            # column 별로 불러오기\n",
    "            inp, mask, inverse_token_mask, token_target, nsp_target = value\n",
    "\n",
    "            # Gradient를 Reset 하는 이유 \n",
    "            # we typically want to explicitly set the gradients to zero \n",
    "            # before starting to do backpropragation \n",
    "            # (i.e., updating the Weights and biases) \n",
    "            # because PyTorch accumulates the gradients on subsequent backward passes. \n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            token,nsp = self.model(inp,mask)\n",
    "\n",
    "            # token mask를 token과 같은 embedding size로 만들기 \n",
    "            tm = inverse_token_mask.unsqueeze(-1).expand_as(token)\n",
    "\n",
    "            # False인 경우 0으로 변환 \n",
    "            token = token.masked_fill(tm,0)\n",
    "\n",
    "            \n",
    "            loss_token = self.ml_criterion(token.transpose(1,2), token_target)\n",
    "            loss_nsp = self.criterion(nsp, nsp_target)\n",
    "\n",
    "            loss = loss_token + loss_nsp\n",
    "            average_mlm_loss += loss_nsp\n",
    "            average_nsp_loss += loss_token\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if index % self._print_every == 0:  \n",
    "                elapsed = time.gmtime(time.time() - prev)  \n",
    "                s = self.training_summary(elapsed, index, average_nsp_loss, average_mlm_loss)  \n",
    "  \n",
    "                if index % self._accuracy_every == 0:  \n",
    "                     s += self.accuracy_summary(index, token, nsp, token_target, nsp_target,inverse_token_mask)\n",
    "  \n",
    "                print(s)  \n",
    "\n",
    "            average_nsp_loss = 0\n",
    "            average_mlm_loss = 0\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def save_checkpoint(self, epoch, step, loss):\n",
    "        # Epoch 저장 \n",
    "        if not self.checkpoint_dir:\n",
    "            return\n",
    "\n",
    "        prev = time.time()\n",
    "        name = f\"bert_epoch{epoch}_step{step}_{datetime.utcnow().timestamp():.0f}.pt\"\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, self.checkpoint_dir.joinpath(name))\n",
    "\n",
    "        print()\n",
    "        print('=' * self._splitter_size)\n",
    "        print(f\"Model saved as '{name}' for {time.time() - prev:.2f}s\")\n",
    "        print('=' * self._splitter_size)\n",
    "        print()\n",
    "\n",
    "    def training_summary(self, elapsed, index, average_nsp_loss, average_mlm_loss):\n",
    "        passed = self.percentage(self.batch_size, self._ds_len, index)\n",
    "        global_step = self.current_epoch * len(self.loader) + index\n",
    "\n",
    "        print_nsp_loss = average_nsp_loss / self._print_every\n",
    "        print_mlm_loss = average_mlm_loss / self._print_every\n",
    "\n",
    "        s = f\"{time.strftime('%H:%M:%S', elapsed)}\"\n",
    "        s += f\" | Epoch {self.current_epoch + 1} | {index} / {self._batched_len} ({passed}%) | \" \\\n",
    "             f\"NSP loss {print_nsp_loss:6.2f} | MLM loss {print_mlm_loss:6.2f}\"\n",
    "\n",
    "        self.writer.add_scalar(\"NSP loss\", print_nsp_loss, global_step=global_step)\n",
    "        self.writer.add_scalar(\"MLM loss\", print_mlm_loss, global_step=global_step)\n",
    "        return s\n",
    "    \n",
    "    def percentage(self,batch_size: int, max_index: int, current_index: int):\n",
    "        \"\"\"Calculate epoch progress percentage\n",
    "        Args:\n",
    "            batch_size: batch size\n",
    "            max_index: max index in epoch\n",
    "            current_index: current index\n",
    "        Returns:\n",
    "            Passed percentage of dataset\n",
    "        \"\"\"\n",
    "        batched_max = max_index // batch_size\n",
    "        return round(current_index / batched_max * 100, 2)\n",
    "\n",
    "    def accuracy_summary(self, index, token, nsp, token_target, nsp_target, inverse_token_mask):\n",
    "        global_step = self.current_epoch * len(self.loader) + index\n",
    "        nsp_acc = self.nsp_accuracy(nsp, nsp_target)\n",
    "        token_acc = self.token_accuracy(token, token_target, inverse_token_mask)\n",
    "\n",
    "        self.writer.add_scalar(\"NSP train accuracy\", nsp_acc, global_step=global_step)\n",
    "        self.writer.add_scalar(\"Token train accuracy\", token_acc, global_step=global_step)\n",
    "\n",
    "        return f\" | NSP accuracy {nsp_acc} | Token accuracy {token_acc}\"\n",
    "\n",
    "    def nsp_accuracy(self, result: torch.Tensor, target: torch.Tensor):\n",
    "        \"\"\"Calculate NSP accuracy between two tensors\n",
    "        Args:\n",
    "            result: result calculated by model\n",
    "            target: real target\n",
    "        Returns:\n",
    "            NSP accuracy\n",
    "        \"\"\"\n",
    "        s = (result.argmax(1) == target.argmax(1)).sum()\n",
    "        return round(float(s / result.size(0)), 2)\n",
    "\n",
    "\n",
    "    def token_accuracy(self, result: torch.Tensor, target: torch.Tensor, inverse_token_mask: torch.Tensor):\n",
    "        \"\"\"Calculate MLM accuracy between ONLY masked words\n",
    "        Args:\n",
    "            result: result calculated by model\n",
    "            target: real target\n",
    "            inverse_token_mask: well-known inverse token mask\n",
    "        Returns:\n",
    "            MLM accuracy\n",
    "        \"\"\"\n",
    "        r = result.argmax(-1).masked_select(~inverse_token_mask)\n",
    "        t = target.masked_select(~inverse_token_mask)\n",
    "        s = (r == t).sum()\n",
    "        return round(float(s / (result.size(0) * result.size(1))), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch.argmax(1)의미\n",
    "* \n",
    "* argmax 중 2차원을 기준으로 가장 큰 값 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22, 32, 77, 25],\n",
      "        [76, 66, 85, 40],\n",
      "        [62, 92, 95,  7]])\n",
      "tensor([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randint(0,100,(3,4))\n",
    "\n",
    "print(a)\n",
    "print(a.argmax(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 491161/491161 [00:05<00:00, 94854.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 전처리 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:39<00:00, 1270.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[646, 30, 7, 2484]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dataset.IMDBBertData(\"./data/IMDB Dataset.csv\", should_include_text=True)\n",
    "\n",
    "a.vocab([\"here\", \"is\", \"the\", \"example\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 시작 0\n",
      "00:00:04 | Epoch 1 | 10 / 73526 (0.01%) | NSP loss   1.14 | MLM loss   0.08\n",
      "00:00:09 | Epoch 1 | 20 / 73526 (0.03%) | NSP loss   1.12 | MLM loss   0.07\n",
      "00:00:13 | Epoch 1 | 30 / 73526 (0.04%) | NSP loss   1.12 | MLM loss   0.07\n",
      "00:00:18 | Epoch 1 | 40 / 73526 (0.05%) | NSP loss   1.12 | MLM loss   0.07\n",
      "00:00:22 | Epoch 1 | 50 / 73526 (0.07%) | NSP loss   1.12 | MLM loss   0.08 | NSP accuracy 0.17 | Token accuracy 0.0\n",
      "00:00:27 | Epoch 1 | 60 / 73526 (0.08%) | NSP loss   1.12 | MLM loss   0.07\n",
      "00:00:31 | Epoch 1 | 70 / 73526 (0.1%) | NSP loss   1.12 | MLM loss   0.08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert copy.ipynb 셀 13\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m BERT \u001b[39m=\u001b[39m Bert(\u001b[39mlen\u001b[39m(a\u001b[39m.\u001b[39mvocab), EMB_SIZE, HIDDEN_SIZE, attention_heads\u001b[39m=\u001b[39m NUM_HEADS)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m trainer \u001b[39m=\u001b[39m BertTrainer(BERT,a,\u001b[39m'\u001b[39m\u001b[39m./data/\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m./data/\u001b[39m\u001b[39m'\u001b[39m, batch_size\u001b[39m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         learning_rate\u001b[39m=\u001b[39m\u001b[39m0.00007\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         epochs\u001b[39m=\u001b[39mEPOCHS)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m trainer()\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert copy.ipynb 셀 13\u001b[0m in \u001b[0;36mBertTrainer.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m) :\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_epoch,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs) :\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_checkpoint(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_epoch)\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert copy.ipynb 셀 13\u001b[0m in \u001b[0;36mBertTrainer.train\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m average_mlm_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_nsp\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m average_nsp_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_token\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EMB_SIZE = 64\n",
    "HIDDEN_SIZE = 36\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 12\n",
    "NUM_HEADS = 4\n",
    "\n",
    "BERT = Bert(len(a.vocab), EMB_SIZE, HIDDEN_SIZE, attention_heads= NUM_HEADS).to('cpu')\n",
    "\n",
    "trainer = BertTrainer(BERT,a,'./data/','./data/', batch_size=BATCH_SIZE,\n",
    "        learning_rate=0.00007,\n",
    "        epochs=EPOCHS)\n",
    "\n",
    "trainer()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
