{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert 소개\n",
    "\n",
    "* Transformer의 encoder 부분만 활용\n",
    "\n",
    "* NLP 분야에 Fine-Tuning 개념 도입\n",
    "* Masked Language Model[MLM] 뿐만아니라 Next Sentence Prediction[NSP]를 통해 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JointEmbedding \n",
    "Bert Embedding 종류는 세가지\n",
    "\n",
    "* Token Embeddings : token을 indices로 변경\n",
    "\n",
    "* Segment Embeddings : 2개 문장의 단어를 구분하기 위해 0,1로 표시 ex) [0,0,0, ... 1,1,1]\n",
    "\n",
    "* Position Embeddings : 전체 단어의 순번 \n",
    "\n",
    "  <img alt='img0' src='./img/img0.png' style=\"width : 400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class JointEmbedding(nn.Module) : \n",
    "\n",
    "    def __init__(self, vocab_size, size, device='cpu') :\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.device = device\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, size)\n",
    "        self.segment_emb = nn.Embedding(vocab_size, size)\n",
    "\n",
    "        self.norm =  nn.LayerNorm(size)\n",
    "\n",
    "    def forward(self,input_tensor) : \n",
    "        # positional embbeding\n",
    "        pos_tensor = self.attention_position(self.size, input_tensor)\n",
    "        # segment embedding\n",
    "        segment_tensor = torch.zeros_like(input_tensor).to(self.device)\n",
    "\n",
    "        # embedding size의 반은 0 반은 1임\n",
    "        sentence_size = input_tensor.size(-1)\n",
    "        segment_tensor[:, sentence_size // 2 + 1:] = 1\n",
    "\n",
    "        output = self.token_emb(input_tensor) + self.segment_emb(segment_tensor) + pos_tensor\n",
    "        return self.norm(output)\n",
    "\n",
    "    def attention_position(self,dim,input_tensor) :\n",
    "        '''\n",
    "        ????\n",
    "        '''\n",
    "        # input_tensor row 크기 \n",
    "        batch_size = input_tensor.size(0)\n",
    "\n",
    "        # 문장 길이\n",
    "        sentence_size = input_tensor(-1)\n",
    "\n",
    "        # pos 정의 longtype = int64\n",
    "        pos = torch.arange(sentence_size, dtype=torch.long).to(self.device)\n",
    "\n",
    "        # d = sentence 내 허용 token 개수\n",
    "        d = torch.arange(dim, dtype=torch.long).to(self.device)\n",
    "        d = (2*d /dim)\n",
    "\n",
    "        # unsqueeze 공부해야할듯..\n",
    "        pos = pos.unsqueeze(1)\n",
    "        pos = pos / (1e4**d)\n",
    "\n",
    "        pos[:, ::2] = torch.sin(pos[:, ::2])\n",
    "        pos[:, 1::2] = torch.cos(pos[:, 1::2])\n",
    "\n",
    "        # *pos는 처음 보는 방식인데\n",
    "        return pos.expand(batch_size, *pos.size())\n",
    "\n",
    "# \n",
    "    def numeric_position(self,dim,input_tensor) : \n",
    "        pos_tensor = torch.arange(dim,dtype=torch.long).to(self.device)\n",
    "        return pos_tensor.expand_as(input_tensor)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert 논문 기본 parameter\n",
    "1. Encoder = 12\n",
    "2. heads = 12\n",
    "3. Hidden Layer(=embedding size) = 768\n",
    "4. word piece = 30522(30522개 단어라는 말)\n",
    "5. Parameter = 110M\n",
    "\n",
    "\n",
    "### 110M parameter 계산하기 \n",
    "* 30522*768 = 24M(embedding 단어)\n",
    "* 12 encoder = 84M\n",
    "    - 1 encoder = 7M\n",
    "    - 세부사항은 상세링크 보기\n",
    "* Dense Weight Matrix and Bias [768, 768] = 589824, [768] = 768, (589824 + 768 = 590592)\n",
    "= 110M\n",
    "\n",
    "    <a href='https://stackoverflow.com/questions/64485777/how-is-the-number-of-parameters-be-calculated-in-bert-model'>상세 링크</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "class Bert(nn.Module) : \n",
    "    def __init__(self,vocab_size,dim_input,dim_output, attention_heads = 12) -> None:\n",
    "        '''\n",
    "        vocab_size : input vocab total\n",
    "        dim_input : (=hidden_layer= embedding_size) 768 \n",
    "        dim_output : (=hidden_layer= embedding_size) 768\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.embedding = JointEmbedding(vocab_size,dim_input)\n",
    "        self.transformerEndoerLayer = nn.TransformerEncoderLayer(d_model=dim_input,nhead=attention_heads,activation='gelu')\n",
    "        # bert Base 12 layer \n",
    "        self.transformerEncoder = nn.TransformerEncoder(self.transformerEndoerLayer,12)\n",
    "        self.token_prediction_layer = nn.Linear(dim_input,vocab_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        # 0 or 1 classification으로 변환\n",
    "        self.classification_layer = nn.Linear(dim_input,2)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask) : \n",
    "        embedded = self.embedding(input_tensor)\n",
    "        encoded = self.transformerEncoder(input_tensor,attention_mask)\n",
    "\n",
    "        token_predictions = self.token_prediction_layer(encoded)\n",
    "\n",
    "        # 모든 행의 첫번째 단어(embedding)\n",
    "        first_word = encoded[:, 0, :]\n",
    "\n",
    "        return self.softmax(token_predictions), self.classification_layer(first_word)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [25, 26, 27, 28, 29],\n",
       "        [50, 51, 52, 53, 54],\n",
       "        [75, 76, 77, 78, 79]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### First_word 표현 방식 이해용\n",
    "import torch.nn\n",
    "import torch\n",
    "\n",
    "a = torch.arange(0,100).reshape((4,5,5))\n",
    "\n",
    "\n",
    "a[:,0,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.sharedctypes import Value\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "# dataset_load\n",
    "import dataset\n",
    "\n",
    "class BertTrainer : \n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        dataset, \n",
    "        log_dir, \n",
    "        checkpoint_dir, \n",
    "        print_progress =10, \n",
    "        print_accuracy = 50,\n",
    "        batch_size=24, \n",
    "        learning_rate = 0.005, \n",
    "        epochs = 5,\n",
    "        device = 'cpu'\n",
    "        ):\n",
    "        self.model = model  \n",
    "        self.dataset = dataset  \n",
    "        self.device = device\n",
    "  \n",
    "        self.batch_size = batch_size  \n",
    "        self.epochs = epochs  \n",
    "        self.current_epoch = 0 \n",
    "\n",
    "        self.loader = DataLoader(self.dataset, batch_size=self.batch_size,shuffle=True) \n",
    "        self.writer = SummaryWriter(str(log_dir))  \n",
    "        self.checkpoint_dir = checkpoint_dir  \n",
    "\n",
    "        # NSP 용        \n",
    "        # This loss combines a Sigmoid layer and the BCELoss in one single class.\n",
    "        # BCE = Binary Cross Entropy    \n",
    "        self.criterion = nn.BCEWithLogitsLoss().to(self.device)  \n",
    "\n",
    "        # MLM 용\n",
    "        # The negative log likelihood loss. It is useful to train a classification problem with C classes.\n",
    "        self.ml_criterion = nn.NLLLoss(ignore_index=0).to(self.device)  \n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.015)\n",
    "    \n",
    "    def train(self, epoch) :\n",
    "        print(f\"epoch 시작 {epoch}\")\n",
    "\n",
    "        prev = time.time()\n",
    "\n",
    "        # gradient set\n",
    "        average_nsp_loss = 0\n",
    "        average_mlm_loss = 0\n",
    "\n",
    "        # Gradient Descent 시작 \n",
    "        for i, value in enumerate(self.loader) :\n",
    "            index = i + 1\n",
    "            inp, mask, inverse_token_mask, token_target, nsp_target = value\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            token,nsp = self.model(inp,mask)\n",
    "\n",
    "            tm = inverse_token_mask.unsqueeze(-1).expand_as(token)\n",
    "            token = token.masked_fill(tm,0)\n",
    "\n",
    "            \n",
    "            loss_token = self.ml_criterion(token.transpose(1,2), token_target)\n",
    "            loss_nsp = self.criterion(nsp, nsp_target)\n",
    "\n",
    "            loss = loss_token + loss_nsp\n",
    "            average_mlm_loss += loss_nsp\n",
    "            average_nsp_loss += loss_token\n",
    "\n",
    "\n",
    "            loss.backword()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if index % self._print_every == 0:  \n",
    "                elapsed = time.gmtime(time.time() - prev)  \n",
    "                s = self.training_summary(elapsed, index, average_nsp_loss, average_mlm_loss)  \n",
    "  \n",
    "            if index % self._accuracy_every == 0:  \n",
    "                s += self.accuracy_summary(index, token, nsp, token_target, nsp_target)  \n",
    "  \n",
    "            print(s)  \n",
    "\n",
    "            # Gradient를 Reset 하는 이유 \n",
    "            # we typically want to explicitly set the gradients to zero \n",
    "            # before starting to do backpropragation \n",
    "            # (i.e., updating the Weights and biases) \n",
    "            # because PyTorch accumulates the gradients on subsequent backward passes. \n",
    "\n",
    "            average_nsp_loss = 0\n",
    "            average_mlm_loss = 0\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def nsp_accuracy(result: torch.Tensor, target: torch.Tensor):\n",
    "        # argmax(1) = 2차원에서 가장 큰 값 \n",
    "        s = (result.argmax(1) == target.argmax(1)).sum()  \n",
    "        return round(float(s / result.size(0)), 2)\n",
    "\n",
    "    def token_accuracy(result: torch.Tensor, target: torch.Tensor, inverse_token_mask: torch.Tensor):\n",
    "        r = result.argmax(-1).masked_select(~inverse_token_mask)  \n",
    "        t = target.masked_select(~inverse_token_mask)  \n",
    "        s = (r == t).sum()  \n",
    "        return round(float(s / (result.size(0) * result.size(1))), 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch.argmax(1)의미\n",
    "* \n",
    "* argmax 중 2차원을 기준으로 가장 큰 값 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6, 71, 84, 83],\n",
      "        [66, 33, 85, 77],\n",
      "        [14, 79,  4, 75]])\n",
      "tensor([2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randint(0,100,(3,4))\n",
    "\n",
    "print(a)\n",
    "print(a.argmax(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert copy.ipynb 셀 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mIMDBBertData(\u001b[39m\"\u001b[39;49m\u001b[39m./data/IMDB Dataset.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, should_include_text\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m a\u001b[39m.\u001b[39mvocab([\u001b[39m\"\u001b[39m\u001b[39mhere\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mis\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mthe\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mexample\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/git_repo/Deep_learning_from_scratch/Bert/dataset.py:37\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, path, ds_from, ds_to, should_include_text)\u001b[0m\n\u001b[1;32m     31\u001b[0m def __init__(\n\u001b[1;32m     32\u001b[0m     self, path, ds_from=None, ds_to=None, should_include_text=False\n\u001b[1;32m     33\u001b[0m ) -> None:\n\u001b[1;32m     34\u001b[0m     \"\"\"\n\u001b[1;32m     35\u001b[0m     should_include_text = True : Debug 모드\n\u001b[1;32m     36\u001b[0m     \"\"\"\n\u001b[0;32m---> 37\u001b[0m     # load dataset\n\u001b[1;32m     38\u001b[0m     self.ds = pd.read_csv(path)[\"review\"]\n\u001b[1;32m     40\u001b[0m     # slice dataset\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "a = dataset.IMDBBertData(\"./data/IMDB Dataset.csv\", should_include_text=True)\n",
    "\n",
    "a.vocab([\"here\", \"is\", \"the\", \"example\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'module' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert copy.ipynb 셀 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m BertTrainer(Bert,dataset,\u001b[39m'\u001b[39;49m\u001b[39m./data/\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m./data/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert copy.ipynb 셀 12\u001b[0m in \u001b[0;36mBertTrainer.__init__\u001b[0;34m(self, model, dataset, log_dir, checkpoint_dir, print_progress, print_accuracy, batch_size, learning_rate, epochs, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs \u001b[39m=\u001b[39m epochs  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_epoch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m DataLoader(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset, batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size,shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m SummaryWriter(\u001b[39mstr\u001b[39m(log_dir))  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Training_Bert%20copy.ipynb#X15sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_dir \u001b[39m=\u001b[39m checkpoint_dir  \n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/utils/data/dataloader.py:353\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# map-style\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[39mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 353\u001b[0m         sampler \u001b[39m=\u001b[39m RandomSampler(dataset, generator\u001b[39m=\u001b[39;49mgenerator)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m         sampler \u001b[39m=\u001b[39m SequentialSampler(dataset)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/utils/data/sampler.py:106\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement, \u001b[39mbool\u001b[39m):\n\u001b[1;32m    103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mreplacement should be a boolean value, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mreplacement=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement))\n\u001b[0;32m--> 106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_samples, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnum_samples should be a positive integer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mvalue, but got num_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/utils/data/sampler.py:114\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnum_samples\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# dataset size might change at runtime\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_samples \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_source)\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_samples\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'module' has no len()"
     ]
    }
   ],
   "source": [
    "\n",
    "BertTrainer(Bert,dataset,'./data/','./data/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
