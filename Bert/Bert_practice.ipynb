{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert 소개\n",
    "\n",
    "* Transformer의 encoder 부분만 활용\n",
    "* NLP 분야에 Fine-Tuning 개념 도입\n",
    "* Masked Language Model[MLM] 뿐만아니라 Next Sentence Prediction[NSP]를 통해 학습\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Next Sentence Prediction** 이해하기\n",
    "\n",
    "* 이전 문장이 현재 문장과 연속적인 내용인지를 판단하는 binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "IMDB_data = pd.read_csv('./data/IMDB Dataset.csv')\n",
    "\n",
    "IMDB_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IMDBBertData' object has no attribute '_preprocess_sentence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb 셀 4\u001b[0m in \u001b[0;36m<cell line: 277>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=265'>266</a>\u001b[0m                 inverse_token_mask \u001b[39m=\u001b[39m inverse_token_mask \u001b[39m+\u001b[39m [\u001b[39mTrue\u001b[39;00m] \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimal_sentence_length \u001b[39m-\u001b[39m len_m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m s, inverse_token_mask\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=276'>277</a>\u001b[0m a \u001b[39m=\u001b[39m IMDBBertData(\u001b[39m'\u001b[39;49m\u001b[39m./data/IMDB Dataset.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,should_include_text\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=278'>279</a>\u001b[0m a\u001b[39m.\u001b[39mvocab([\u001b[39m'\u001b[39m\u001b[39mhere\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mis\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mthe\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mexample\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb 셀 4\u001b[0m in \u001b[0;36mIMDBBertData.__init__\u001b[0;34m(self, path, ds_from, ds_to, should_include_text)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39melse\u001b[39;00m : \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMASKED_INDICES_COLUMN,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTARGET_COLUMN,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTOKEN_MASK_COLUMN,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mNSP_TARGET_COLUMN\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         ]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_dataset()\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb 셀 4\u001b[0m in \u001b[0;36mIMDBBertData.prepare_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(review_sentences) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) :\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     \u001b[39m# True NSP item\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     first, second \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(review_sentences[i]),\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(review_sentences[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m])\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     nsp\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_item(first,second,\u001b[39m1\u001b[39;49m))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     \u001b[39m# False NSP item \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     first,second \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_false_nsp_sentences(sentences)\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb 셀 4\u001b[0m in \u001b[0;36mIMDBBertData._create_item\u001b[0;34m(self, first, second, target)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m \u001b[39mNSP 훈련용으로 활용\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39m1. Masked Sentence 생성\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39m2. random words Sentence 생성\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m \u001b[39m# 1.masked Sentence 생성\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m updated_first, first_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess_sentence(first\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m updated_second, second_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_sentence(second\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Deep_learning_from_scratch/Bert/Bert_practice.ipynb#X12sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m nsp_sentence \u001b[39m=\u001b[39m updated_first \u001b[39m+\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSEP] \u001b[39m+\u001b[39m updated_second\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'IMDBBertData' object has no attribute '_preprocess_sentence'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class IMDBBertData(Dataset) :\n",
    "    # special token\n",
    "    CLS = '[CLS]' # 문장 시작\n",
    "    PAD = '[PAD]' # 빈공간\n",
    "    SEP = '[SEP]' # 문장 끝\n",
    "    MASK = '[MASK]'\n",
    "    UNK = '[UNK]'\n",
    "\n",
    "    # \n",
    "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
    "    TARGET_COLUMN = 'indices'\n",
    "\n",
    "    # nsp 용도\n",
    "    NSP_TARGET_COLUMN = 'is_next'\n",
    "    TOKEN_MASK_COLUMN = 'token_mask'\n",
    "\n",
    "    mask_percent = 0.15\n",
    "\n",
    "    OPTIMAL_LENGTH_PERCENTILE = 70\n",
    "\n",
    "\n",
    "    def __init__(self,path, ds_from=None, ds_to=None, should_include_text=False) -> None:\n",
    "        '''\n",
    "         should_include_text = True : Debug 모드 \n",
    "        '''\n",
    "        # load dataset\n",
    "        self.ds = pd.read_csv(path)['review']\n",
    "        \n",
    "        # slice dataset\n",
    "        if ds_from is not None or ds_to is not None : \n",
    "            self.ds[ds_from:ds_to]\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.counter = Counter()\n",
    "        self.vocab = None\n",
    "\n",
    "        self.optimal_sentence_length = None\n",
    "        self.should_include_text = should_include_text\n",
    "\n",
    "        # self.df(dataframe) 만들 때 column 정의\n",
    "        if should_include_text :\n",
    "            self.columns = [\n",
    "                'masked_sentence',\n",
    "                self.MASKED_INDICES_COLUMN,'sentence',\n",
    "                self.TARGET_COLUMN,\n",
    "                self.TOKEN_MASK_COLUMN,\n",
    "                self.NSP_TARGET_COLUMN\n",
    "                ]\n",
    "\n",
    "        else : \n",
    "            self.columns = [\n",
    "                self.MASKED_INDICES_COLUMN,\n",
    "                self.TARGET_COLUMN,\n",
    "                self.TOKEN_MASK_COLUMN,\n",
    "                self.NSP_TARGET_COLUMN\n",
    "                ]\n",
    "\n",
    "        self.df = self.prepare_dataset()\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ''' \n",
    "        getitem은 객체를 슬라이싱 하는데 활용 됨\n",
    "        '''\n",
    "        pass \n",
    "\n",
    "    def prepare_dataset(self) :\n",
    "        '''\n",
    "        Class Main Function\n",
    "        Bert에 활용 될 Dataset 만드는 function \n",
    "        '''\n",
    "\n",
    "        # vocab에 단어 저장\n",
    "        # vocab 사용법 : vocab(['here','is','the','example'])\n",
    "\n",
    "        sentences = []\n",
    "        nsp = []\n",
    "        sentence_lens = []\n",
    "\n",
    "        for review in self.ds :\n",
    "            review_sentences = review.split('. ')\n",
    "            sentences += review_sentences #extend 대신 + 사용해도 됨\n",
    "\n",
    "            # For MLM\n",
    "            self._update_length(review_sentences,sentence_lens)\n",
    "\n",
    "            # For NSP\n",
    "            if len(review_sentences) > 1 :\n",
    "                for i in range(len(review_sentences) - 1) :\n",
    "                    # True NSP item\n",
    "                    first, second = self.tokenizer(review_sentences[i]),self.tokenizer(review_sentences[i + 1])\n",
    "                    nsp.append(self._create_item(first,second,1))\n",
    "\n",
    "                    # False NSP item \n",
    "                    first,second = self._select_false_nsp_sentences(sentences)\n",
    "                    first,second = self.tokenizer(first), self.tokenizer(second)\n",
    "                    nsp.append(self._create_item(first,second,0))\n",
    "\n",
    "        # For MLM\n",
    "        self.optimal_sentence_length = self._find_optimal_sentence_length(sentence_lens)\n",
    "\n",
    "        for sentence in sentences : \n",
    "            s = self.tokenizer(sentence)\n",
    "            self.counter.update(s)\n",
    "        self._fill_vocab()\n",
    "\n",
    "        # For NSP\n",
    "        df = pd.DataFrame(nsp,columns=self.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # For MLM\n",
    "    def _update_length(self,input:list, output) :\n",
    "        '''\n",
    "        개별 문장의 단어 개수 반환\n",
    "        '''\n",
    "        for sen in input :\n",
    "            output.append(len(sen.split(' '))) \n",
    "\n",
    "        \n",
    "\n",
    "    def _find_optimal_sentence_length(self,lengths:list) :\n",
    "        '''\n",
    "        lengths = dataset에 있는 문장 길이\n",
    "        return : 70% 범위의 문장 길이\n",
    "        IMDB는 27개라고 함.\n",
    "        '''\n",
    "        arr = np.array(lengths)\n",
    "        return int(np.percentile(arr,self.OPTIMAL_LENGTH_PERCENTILE))\n",
    "\n",
    "    def _fill_vocab(self) :\n",
    "        # 2번 이상 반복되는 단어만 저장하기\n",
    "        self.vocab = vocab(self.counter, min_freq=2)  \n",
    "\n",
    "        self.vocab.insert_token(self.CLS, 0)  \n",
    "        self.vocab.insert_token(self.PAD, 1)  \n",
    "        self.vocab.insert_token(self.MASK, 2)  \n",
    "        self.vocab.insert_token(self.SEP, 3)  \n",
    "        self.vocab.insert_token(self.UNK, 4)  \n",
    "        self.vocab.set_default_index(4)\n",
    "\n",
    "    def _create_item(self,first:list, second:list, target:int = 1):\n",
    "        '''\n",
    "        NSP 훈련용으로 활용\n",
    "        1. Masked Sentence 생성\n",
    "        2. random words Sentence 생성\n",
    "        '''\n",
    "\n",
    "        # 1.masked Sentence 생성\n",
    "        updated_first, first_mask = self._preprocess_sentence(first.copy())\n",
    "        updated_second, second_mask = self._preprocess_sentence(second.copy())\n",
    "\n",
    "        nsp_sentence = updated_first + [self.SEP] + updated_second\n",
    "        nsp_indicies = self.vocab.lookup_indices(nsp_sentence)\n",
    "        inverse_token_mask = first_mask + [True] + second_mask\n",
    "\n",
    "        # 2.masked Sentence 생성\n",
    "        first,_ =self._preprocess_sentence(first.copy(), should_mask = False)\n",
    "        second,_ =self._preprocess_sentence(second.copy(), should_mask = False)\n",
    "        original_nsp_sentence = first + [self.SEP] + second \n",
    "        original_nsp_indices = self.vocab.lookup_indices(original_nsp_sentence)\n",
    "\n",
    "        if self.should_include_text :\n",
    "            return(\n",
    "                nsp_sentence,\n",
    "                nsp_indicies,\n",
    "                original_nsp_sentence,\n",
    "                original_nsp_indices,\n",
    "                inverse_token_mask,\n",
    "                target\n",
    "            )\n",
    "        else : \n",
    "            return (\n",
    "                nsp_indicies,\n",
    "                original_nsp_indices,\n",
    "                inverse_token_mask,\n",
    "                target\n",
    "            )\n",
    "\n",
    "    # For NSP Sentence\n",
    "    def _select_false_nsp_sentences(self, sentences:list) :\n",
    "        ''' \n",
    "        false NSP 만들 sentence 선택하기\n",
    "        sentences : 문장 리스트 전체\n",
    "        return : 임의로 문장 2개 선택(앞과 뒤는 연결되지 않음)\n",
    "        '''\n",
    "        sentences_len = len(sentences)\n",
    "        sentence_index = torch.randint(0,sentences_len -1)\n",
    "        next_sentence_index = torch.randint(0,sentences_len -1)\n",
    "\n",
    "        while next_sentence_index == sentence_index +1 : \n",
    "            next_sentence_index = torch.randint(0,sentences_len -1)\n",
    "            \n",
    "        return sentences[sentence_index], sentences[next_sentence_index]\n",
    "\n",
    "    def _preprocess_sentence(self, sentence:list, should_mask :bool) :\n",
    "        ''' \n",
    "        mask 퍼센트(15%)를 [mask]로 바꾸는 매소드\n",
    "        sentences : 문장 리스트 전체\n",
    "        return : mask 된 문장, inverse token mask\n",
    "        '''\n",
    "\n",
    "        inverse_token_mask = None\n",
    "        if should_mask : \n",
    "            sentence, inverse_token_mask = self._mask_sentence(sentence)\n",
    "        sentence, inverse_token_mask = self._pad_sentence([self.CLS],sentence,inverse_token_mask)\n",
    "\n",
    "        return sentence, inverse_token_mask\n",
    "\n",
    "    def _mask_sentence(self,sentence:list) :\n",
    "        ''' \n",
    "        mask 퍼센트(15%)를 [mask] 또는 랜덤한 단어로 바뀜\n",
    "        이때 [mask] : random_word = 8 : 2 비율\n",
    "        sentence : 문장 하나\n",
    "        return : mask 또는 random 단어 변환 된 문장, inverse token mask\n",
    "        '''\n",
    "        len_s = len(sentence)\n",
    "        # len(len_s, 27)\n",
    "        # masked 된 문장 또는 단어가 바뀐 경우 False로 변환됨\n",
    "        inverse_token_mask = [True for _ in range(max(len_s,self.optimal_sentence_length))] \n",
    "\n",
    "        mask_amount = round(len_s *self.mask_percent) \n",
    "\n",
    "        for _ in range(mask_amount) : \n",
    "            i = torch.randint(0,len_s - 1)\n",
    "\n",
    "            if random.random() < 0.8 :\n",
    "                # mask_amount의 80%는 mask로 \n",
    "                sentence[i] = self.MASK\n",
    "            else :\n",
    "                # # mask_amount의 20%는 단어 바꾸기 \n",
    "                # 5인 이유는 0,1,2,3,4 토큰이 모두 special token이기 때문\n",
    "                j = torch.randint(5,len(self.vocab) - 1)\n",
    "\n",
    "                # 단어 바꾸기\n",
    "                sentence[i] = self.vocab.lookup_token(j)\n",
    "\n",
    "            inverse_token_mask[i] = False\n",
    "\n",
    "        return sentence, inverse_token_mask\n",
    "\n",
    "    def _pad_sentence(self,sentence : list,inverse_token_mask : bool) :\n",
    "        len_s =len(sentence)\n",
    "\n",
    "        # self.optimal_sentence_length = 27단어\n",
    "        if len_s >= self.optimal_sentence_length :\n",
    "            s = sentence[:self.optimal_sentence_length]\n",
    "        else : \n",
    "            s = sentence + [self.PAD] * (self.optimal_sentence_length - len_s)\n",
    "        \n",
    "        if inverse_token_mask :\n",
    "            len_m = len(inverse_token_mask) \n",
    "            if len_m >= self.optimal_sentence_length :\n",
    "                inverse_token_mask = inverse_token_mask[:self.optimal_sentence_length]\n",
    "            else : \n",
    "                inverse_token_mask = inverse_token_mask + [True] * (self.optimal_sentence_length - len_m)\n",
    "\n",
    "        return s, inverse_token_mask\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = IMDBBertData('./data/IMDB Dataset.csv',should_include_text=True)\n",
    "\n",
    "a.vocab(['here','is','the','example'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[95, 198, 59, 65, 154]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset= pd.read_csv('./data/IMDB Dataset.csv')\n",
    "\n",
    "sen_list = []\n",
    "\n",
    "for review in dataset['review'] :\n",
    "    sentences = review.split('. ')\n",
    "    sen_list += sentences\n",
    "\n",
    "sen_len = [len(sen) for sen in sen_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
