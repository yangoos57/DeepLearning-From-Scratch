{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder와 Decoder 구현\n",
    "\n",
    "![img1](./img/img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 세부 구조\n",
    "\n",
    "* selfattention 레이어 끼리는 서로 연관성이 있지만 feed forward 사이에는 개별성이 있음. \n",
    "* feed forward는 개별성이 있기 때문에 parallel이 가능한 구조가 됨.\n",
    "\n",
    "![img3](./img/img3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi_Headed_Attention\n",
    "\n",
    "* Multi_Headed_Attention = Heads * SelfAttention\n",
    "\n",
    "* embeding_size = Token 개수 | parameter\n",
    "\n",
    "* Heads = Attention 개수 | parameter\n",
    "\n",
    "* Attention_dim = embeding_size // Heads\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention 구현\n",
    "* **단어와의 연관성, 중요도를 계산하는 레이어**\n",
    "\n",
    "    ![img4](./img/img4.png)\n",
    "\n",
    "* embed_size : 토큰 개수 , 논문에서는 512개로 사용\n",
    "\n",
    "    ![img2](./img/img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query, Key, Value \n",
    "\n",
    "* self attention이 단어와의 연관성을 파악하기 위해서 query, key, value라는 개념을 사용한다.\n",
    "    - query : 현재 관심을 갖는 단어 단어 | vector\n",
    "    - key : column 명이라고 보면 될듯 vector | vector\n",
    "    - value : column 안에 있는 실제 값인 듯 vector | vector \n",
    "\n",
    "    ![img5](./img/img5.png)\n",
    "\n",
    "* Score = query와 key의 곱을 한뒤 attention 차원의 루트 값으로 나눈다.\n",
    "    \n",
    "    $scaled dot product attention = score(p,k) = \\frac{Q K^\\text{T}}{\\sqrt{d}}$\n",
    "\n",
    "* Back Propagation을 통해 query, key, value의 W을 찾아야한다.\n",
    "    - forward Propagation 예시 : $embedding(1*4) * W^Q (4*3) = q_1(1*3)$\n",
    "    - 위 식을 통해서 back propagation을 구현해서 만든다. \n",
    "    - \n",
    "\n",
    "    ![img6](./img/img6.png)\n",
    "\n",
    "    - query, key, value는 각각 64차원 * 8(multi-headed-attention 개수 ) = embed_size\n",
    "    - head가 8개라는건 양옆 4개 단어를 비교하겠다는건가..?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query, key, value W를 계산하는 절차\n",
    "\n",
    "* Step 1: Create three vectors from each of the encoder’s input vectors\n",
    "\n",
    "    `head_dim = embed_size // heads`\n",
    "\n",
    "* Step 2: Calculate a score\n",
    "\n",
    "    * 처음에는 W값들이 임의로 구해지나?\n",
    "\n",
    "\n",
    "* Step 3: Divide the score by  $\\sqrt{b_k}$\n",
    "    이렇게 해야 성능이 잘나온다고 논문에 나옴.\n",
    "\n",
    "* Step 4: Pass the result through a softmax operation\n",
    "    지금 보고 있는 토큰과 얼마나 관련성이 있는지 확률로 변환\n",
    "\n",
    "* Step 5: Multiply each value vector by the softmax score\n",
    "    v에다가 퍼센트를 곱해서 단어의 중요도를 반영\n",
    "\n",
    "* Step 6 : Sum up the weighted value vector which produces the output of the self- attention layer at this position\n",
    "    Step 5에서 얻은 값들을 모두 더하면 self attention 값이 된다.\n",
    "\n",
    "\n",
    "단계 한눈에 보기\n",
    "\n",
    "![img7](./img/img7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 계산 절차를 병렬로 계산하기 (Matrix calulation of self attention)\n",
    "\n",
    "$\\begin{equation}\n",
    " Attention(Q,K,V)= Z = \\mathrm{softmax} \\left( \\frac{Q K^\\text{T}}{\\sqrt{d}} \\right) V\n",
    "\\end{equation}$\n",
    "\n",
    "![img8](./img/img8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-headed attention\n",
    "\n",
    "* attention($z_n$) 결과 값을 모두 concat($z_1 + z_2 + ... + z_8 $) = $z_{all}$\n",
    "* $ z_{all} * W^0 = Z $\n",
    "\n",
    "    ![img9](./img/img9.png)\n",
    "\n",
    "### Z는 Feed Forward에 투입되는 값이다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class selfAttention(nn.Module) :\n",
    "    def __init__(self,embed_size, heads) -> None:\n",
    "        '''\n",
    "        embed_size : input 토큰 개수, 논문에서는 512개로 사용 \n",
    "        heads : multi_head의 개수, 논문에서는 8개 사용\n",
    "        \n",
    "        '''\n",
    "        # nn.Module의 parameter 불러오기\n",
    "        super(selfAttention,self).__init__() \n",
    "        # 왜 selfAttention을 또 불러오는지 모르겠네..\n",
    "        # 성능상 이슈는 아니고 참고용으로 사용할때 도움되는거라고 함.\n",
    "\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size //heads\n",
    "        \n",
    "        # Linear 함수느 어떤 용도로 쓰이는거지?\n",
    "        '''\n",
    "        values :\n",
    "        keys :\n",
    "        queries : \n",
    "        '''\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "        \n",
    "    def forward(self, values,keys,query,mask) :\n",
    "        N = query.shape[0] \n",
    "        # tensor shape\n",
    "        value_len = values.shape[1] \n",
    "        key_len = keys.shape[1] \n",
    "        query_len = query.shape[1]\n",
    "\n",
    "        values = values.reshape(N,value_len, self.heads,self.head_dim)\n",
    "        keys = keys.reshape(N,key_len, self.heads,self.head_dim)\n",
    "        queries = queries.reshape(N,query_len, self.heads,self.head_dim)\n",
    "\n",
    "        # score 설명 참고\n",
    "        # einsum이 뭐지\n",
    "        score = torch.einsum(\"nqhd,nkhd->nhqk\", [queries,keys]) \n",
    "        # queries shape : N,value_len, self.heads,self.head_dim\n",
    "        # keys shape : N,key_len, self.heads,self.head_dim\n",
    "        # score shape : N, heads, query_len, key_len\n",
    "        \n",
    "        # decoder 구조인 masked Self Attention 적용 시 활용되는 구문\n",
    "        # decoder 설명시 상세 소개\n",
    "        if mask is not None :\n",
    "            score = score.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "            '''\n",
    "            -1e20 = -inf\n",
    "            -inf이기 때문에 값이 0에 수렴(0은 안됨)\n",
    "            mask가 부여된 경우 score 값을 0으로 준다.\n",
    "\n",
    "            '''\n",
    "\n",
    "        attention = torch.softmax(score / self.embed_size**(1/2),dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd -> nqhd\",[attention, values]).reshape(\n",
    "            N,query_len,self.heads,self.head_dim\n",
    "            )\n",
    "        # attention shape : N, heads,query_len,key_len\n",
    "        # values shape : N, value_len, heads, heads_dim\n",
    "        # out shape : N, query_len, heads, head_dim\n",
    "\n",
    "        # concat all heads \n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add(=residual connection) & Norm\n",
    "* add(=residual connection)를 하는 이유는 gradient descent 할 때 0이 될 수 있기 때문\n",
    "* 방법 ; z 에다가 x를 더한다. 이때 x 는 이전 z 값 z는 현재 z값\n",
    "\n",
    "\n",
    "\n",
    "### LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PointWize Feed Forward\n",
    "* 같은 계층의 encoder에서 feed forward 구조가 같음\n",
    "* Linear(512d)->relu(2048d) -> Linear(512d)\n",
    "\n",
    "![img10](./img/img10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformerBlock(nn.Module) :\n",
    "    def __init__(self,embed_size, heads, dropout, forward_expansion) -> None:\n",
    "        '''\n",
    "        embed_size : token 개수 | 논문 512개\n",
    "        heads : attention 개수\n",
    "        dropout :\n",
    "        forward_expansion : forward 계산시 차원을 얼마나 늘릴 것인지 결정, 임의로 결정하는 값\n",
    "                            forward_차원 계산은 forward_expension * embed_size \n",
    "                            논문에서는 4로 정함. 총 2048차원으로 늘어남.\n",
    "\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.attention = selfAttention(embed_size,heads)\n",
    "        \n",
    "        # LayerNorm 장점 및 적용하는 이유는?\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forawrd = nn.Sequential(\n",
    "            # 차원을 512 -> 2048로 증가\n",
    "            nn.Linear(embed_size,forward_expansion*embed_size),\n",
    "            # 차원을 Relu 연산\n",
    "            nn.ReLU(),\n",
    "            # 차원 2048 -> 512로 축소 \n",
    "            nn.Linear(forward_expansion*embed_size,embed_size)\n",
    "            )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value,key,query,mask) :\n",
    "        # 왜 attention.forward가 아니고 그냥 쓰는거지?\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "\n",
    "        forward = self.feed_forawrd(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
